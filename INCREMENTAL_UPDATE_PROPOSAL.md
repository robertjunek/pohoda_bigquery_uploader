# ğŸ’¡ Implementace Incremental Update

## ğŸ” SouÄasnÃ½ problÃ©m

VÃ¡Å¡ skript **NEÅ˜EÅ Ã skuteÄnÃ½ incremental update**:

1. **âŒ PÅ™episuje celÃ© tabulky** - pouÅ¾Ã­vÃ¡ `WRITE_TRUNCATE`
2. **âŒ Stahuje mnoho dat zbyteÄnÄ›** - `days_back: 4000` = cca 11 let
3. **âŒ Å½Ã¡dnÃ¡ logika pro UPDATE** - jen INSERT celÃ© tabulky znovu
4. **âŒ NepouÅ¾Ã­vÃ¡ ID pro detekci zmÄ›n**

## âœ… Co by mÄ›l dÄ›lat skuteÄnÃ½ incremental update

### 1. **Detekce zmÄ›nÄ›nÃ½ch zÃ¡znamÅ¯**
```sql
-- MÃ­sto tohoto (souÄasnÃ½ stav):
WHERE COALESCE(h.DatSave, h.DatCreate ) >= GETDATE() - 4000

-- MÄ›lo by bÃ½t toto:
WHERE COALESCE(h.DatSave, h.DatCreate) > @last_sync_timestamp
OR h.ID IN (SELECT id FROM changed_records)
```

### 2. **MERGE mÃ­sto TRUNCATE**
```sql
-- BigQuery MERGE statement
MERGE dataset.table_name T
USING temp_new_data S
ON T.ID = S.ID
WHEN MATCHED THEN UPDATE SET *
WHEN NOT MATCHED THEN INSERT *
```

### 3. **Tracking poslednÃ­ho synchronizaÄnÃ­ho Äasu**
```python
# UloÅ¾enÃ­ timestamp poslednÃ­ synchronizace
def save_last_sync_time(table_name: str, timestamp: datetime):
    # UloÅ¾it do metadata tabulky nebo samostatnÃ© tabulky
```

## ğŸš€ NÃ¡vrh implementace

### Krok 1: PÅ™idÃ¡nÃ­ sync metadata

```python
def create_sync_metadata_table(self):
    """VytvoÅ™Ã­ tabulku pro tracking synchronizace."""
    query = """
    CREATE TABLE IF NOT EXISTS `{project}.{dataset}._sync_metadata` (
        table_name STRING,
        last_sync_timestamp TIMESTAMP,
        last_sync_max_id STRING,
        records_synced INT64,
        sync_mode STRING,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
    )
    """
```

### Krok 2: Modifikace SQL dotazÅ¯

```python
def modify_sql_for_incremental(self, sql_content: str, table_name: str) -> str:
    """UpravÃ­ SQL pro incremental sync."""
    
    # ZÃ­skej poslednÃ­ sync timestamp
    last_sync = self.get_last_sync_timestamp(table_name)
    
    if last_sync:
        # PÅ™idej podmÃ­nku pro jen novÃ©/upravenÃ© zÃ¡znamy
        incremental_condition = f"""
        AND (
            COALESCE(h.DatSave, h.DatCreate) > '{last_sync}'
            OR h.ID > (SELECT COALESCE(MAX(last_max_id), 0) FROM sync_metadata WHERE table_name = '{table_name}')
        )
        """
        sql_content = sql_content.replace(
            'WHERE r.Kod IS NOT NULL',
            f'WHERE r.Kod IS NOT NULL {incremental_condition}'
        )
    
    return sql_content
```

### Krok 3: MERGE mÃ­sto TRUNCATE

```python
def upload_incremental(self, df: pd.DataFrame, table_name: str):
    """Nahraje data s MERGE logikou."""
    
    # 1. VytvoÅ™ doÄasnou tabulku
    temp_table = f"temp_{table_name}_{int(time.time())}"
    
    # 2. Nahraj data do temp tabulky
    self.upload_to_temp_table(df, temp_table)
    
    # 3. ProveÄ MERGE
    merge_query = f"""
    MERGE `{self.project}.{self.dataset}.{table_name}` T
    USING `{self.project}.{self.dataset}.{temp_table}` S
    ON T.ID = S.ID
    WHEN MATCHED THEN 
        UPDATE SET * EXCEPT(ID)
    WHEN NOT MATCHED THEN 
        INSERT *
    """
    
    # 4. SpusÅ¥ MERGE
    self.bq_client.query(merge_query).result()
    
    # 5. SmaÅ¾ temp tabulku
    self.drop_temp_table(temp_table)
```

## ğŸ“‹ Konfigurace pro incremental

### RozÅ¡Ã­Å™it config.json:

```json
{
  "sync": {
    "mode": "incremental",  // "full" nebo "incremental"
    "batch_size": 5000,
    "days_back": 7,         // Pro first run nebo full sync
    "incremental_field": "DatSave",  // Pole pro tracking zmÄ›n
    "merge_strategy": "upsert"       // "upsert", "append", "replace"
  }
}
```

## ğŸ› ï¸ Implementace do souÄasnÃ©ho kÃ³du

### 1. Upravit upload_to_bigquery metodu:

```python
def upload_to_bigquery(self, df: pd.DataFrame, table_name: str, batch_size: int):
    """UpravenÃ¡ verze s incremental podporou."""
    
    sync_mode = self.config['sync'].get('mode', 'full')
    
    if sync_mode == 'incremental' and self.table_exists(table_name):
        # Incremental update
        self.upload_incremental(df, table_name)
        self.update_sync_metadata(table_name, df)
    else:
        # Full refresh (souÄasnÃ¡ logika)
        self.upload_full_refresh(df, table_name, batch_size)
```

### 2. SnÃ­Å¾it days_back pro bÄ›Å¾nÃ½ provoz:

```python
# V _add_table_prefix metodÄ›:
if self.sync_mode == 'incremental':
    days_back = 3  # Jen poslednÃ­ 3 dny pro bezpeÄnost
else:
    days_back = self.config['sync'].get('days_back', 14)
```

## âš¡ VÃ½hody incremental update

1. **ğŸš€ Rychlost** - stahuje jen novÃ¡/zmÄ›nÄ›nÃ¡ data
2. **ğŸ’¾ Ãšspora zdrojÅ¯** - mÃ©nÄ› dat pÅ™es sÃ­Å¥
3. **ğŸ“Š HistorickÃ© zÃ¡znamy** - nemazÃ¡nÃ­m celÃ© tabulky
4. **ğŸ”„ ÄŒastÄ›jÅ¡Ã­ sync** - mÅ¯Å¾e bÄ›Å¾et kaÅ¾dou hodinu
5. **ğŸ’° NiÅ¾Å¡Ã­ nÃ¡klady** - mÃ©nÄ› BigQuery operacÃ­

## ğŸ“ DoporuÄenÃ­

### OkamÅ¾itÃ©:
```bash
# SniÅ¾te days_back pro souÄasnÃ½ provoz
"days_back": 7   # mÃ­sto 4000
```

### DlouhodobÃ©:
1. Implementujte MERGE logiku
2. PÅ™idejte sync metadata tracking
3. Nastavte incremental mode jako default
4. PÅ™idejte monitoring a alerting

## ğŸ§ª TestovÃ¡nÃ­

```bash
# Test full sync
make test-sync

# Test incremental (po implementaci)
make test-sync-incremental

# Monitoring velikosti dat
make sync-stats
```

---

**ZÃ¡vÄ›r:** SouÄasnÃ½ kÃ³d dÄ›lÃ¡ jen "full refresh", ne skuteÄnÃ½ incremental update. Pro efektivnÃ­ provoz byste mÄ›li implementovat MERGE logiku a metadata tracking.